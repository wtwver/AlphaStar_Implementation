
Search
Write
Sign up

Sign in



AlphaStar implementation series - Replay file
Dohyeong Kim
Dohyeong Kim

·
Follow

2 min read
·
Jun 8, 2020
12





I recently make a simple Terran agent using Rule-based system using PySC2 of the DeepMind. I can make it using conditional statements up up to producing Marauder and rushing to enemy base. However, I realize that program will become too complicated to make and control more high tech units.

For that reason, I decide to use the Deep Learning method instead of the Rule-based method. Therefore, I started to read the AlphaStar paper, which shows the best performance in Starcraft2 game.

I also tried to replicate the AlphaGo paper published in few years ago. However, I failed because there was not many resources such as open source environment, sample code of other people.

On the other side, AlphaStar has various abundant resources. Therefore, I can start to replicate paper at this time.

API for downloading replay file: https://github.com/Blizzard/s2client-proto/tree/master/samples/replay-api
API for parsing replay file : https://github.com/narhen/pysc2-replay
Parsing replay file
In the Replay file, the first information we should check is the mmr, apm, win or lose of each player.


Players information of replay file
Through the code above, we can find a replay files that meet certain player condition.

When a replay file is selected, we can prepare to extract the state and action occurred during the game, via the following code.

Data parsing preparation from replay file
Finally, we need to to save the parsing data as an hkl file format. In the case of action, the type and argument are separated from original action data. In the case of observation, feature_screen, feature_minimap, player, feature_units, game_loop, available_actions, build_queue, production_queue, single_select, multi_select and score_cumulative are separated.

Recording data of replay file
You can see that the size of the original replay file of Starcraft 2 is around 13kB, and the size of the hkl file generated from it is about 600MB.

You can see full source code from https://github.com/kimbring2/AlphaStar_Implementation/blob/master/trajectory_generator.py.

Conclusion
In the first post of the series, we check how to extract human expert data from Starcraft 2 replay files, which is essential for training the Deep Learning agent of Starcraft 2. In the next post, I will explain how to build a network for agents.

Reinforcement Learning
Starcraft 2
Deep Learning
Alphastar
Deepmind
12



Dohyeong Kim
Written by Dohyeong Kim
104 Followers
·
97 Following
I am a Deep Learning researcher. Currently, I am trying to make an AI agent for various situations such as MOBA, RTS, and Soccer games.

Follow

No responses yet
What are your thoughts?

Cancel
Respond
Respond

Also publish to my profile

More from Dohyeong Kim
Implementation of the Hide and Seek of the OpenAI — Part 1
Dohyeong Kim
Dohyeong Kim

Implementation of the Hide and Seek of the OpenAI — Part 1
Collaboration is an essential function of multiplayer game such as a MOBA, and Soccer game. In the case of Reinforcement Learning, the…
Mar 12, 2023
3
1
How to build the Deep Learning agent for Minecraft with code— Tutorial 2
Dohyeong Kim
Dohyeong Kim

How to build the Deep Learning agent for Minecraft with code— Tutorial 2
In the previous post, I created two model needed to create Deep Learning agent for Minecraft.
Mar 5, 2022
63
Image to Latex using Vision Transformer
Dohyeong Kim
Dohyeong Kim

Image to Latex using Vision Transformer
Introduction
Feb 13, 2024
101
3
AlphaStar implementation series — part5
Dohyeong Kim
Dohyeong Kim

AlphaStar implementation series — part5
Introduction
Nov 15, 2020
8
See all from Dohyeong Kim
Recommended from Medium
Introduction to Portfolio Management using Python (5)
Coinmonks
In

Coinmonks

by

MicroBioscopicData (by Alexandros Athanasopoulos)

Introduction to Portfolio Management using Python (5)
Reduce Portofolio Risk by Diversification

Sep 27, 2024
108
Mastering Quant Trading with Proximal Policy Optimization in Deep Reinforcement Learning
Funny AI & Quant
In

Funny AI & Quant

by

Pham The Anh

Mastering Quant Trading with Proximal Policy Optimization in Deep Reinforcement Learning
Unlock the Power of Proximal Policy Optimization in Deep Reinforcement Learning to Revolutionize Your Quant Trading Strategies

Sep 20, 2024
1
Lists
Large Reasoning Models (LRMs) learn complex reasoning with minimal data using the Less-Is-More Reasoning Hypothesis (LIMO). Long chain-of-thought (Long CoT) reasoning is efficiently trained with supervised fine-tuning (SFT) and LoRA. The LIMO model achieves 57.1% on AIME and 94.8% on MATH with just 817 samples, redefining AI training efficiency. Access the open-source LIMO suite for cutting-edge research.


Natural Language Processing
1946 stories
·
1599 saves
Principal Component Analysis for ML
Time Series Analysis
deep learning cheatsheet for beginner
Practical Guides to Machine Learning
10 stories
·
2208 saves



data science and AI
40 stories
·
333 saves


Staff picks
815 stories
·
1626 saves
I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market
DataDrivenInvestor
In

DataDrivenInvestor

by

Austin Starks

I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market
It literally took one try. I was shocked.

Sep 16, 2024
8.9K
231
Reinforcement Learning
Aishwarya
Aishwarya

Reinforcement Learning
Everything you need to know to get started with Reinforcement Learning
Oct 1, 2024
1
Some Fragmented Thoughts on LLMs
Sissi Feng (day day up)
Sissi Feng (day day up)

Some Fragmented Thoughts on LLMs
Large language models (LLMs) like GPT have made remarkable strides in generating human-like text and facilitating various applications in…
Sep 13, 2024
A Beginner’s Guide to Q-Learning: Understanding with a Simple Gridworld Example
Gregory Kovalchuk
Gregory Kovalchuk

A Beginner’s Guide to Q-Learning: Understanding with a Simple Gridworld Example
Reinforcement learning (RL) is one of the most exciting fields in machine learning, allowing agents to learn optimal behaviors in uncertain…
Oct 24, 2024
3
See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Terms

Text to speech

Teams