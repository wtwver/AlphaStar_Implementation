
Search
Write
Sign up

Sign in



AlphaStar implementation series — Supervised Learning
Dohyeong Kim
Nerd For Tech
Dohyeong Kim

·
Follow

Published in
Nerd For Tech

·
4 min read
·
Sep 4, 2021
15





In the past post, I showed how to solve one of the minigames of PySC2 using the Actor-Critic Reinforcement Learning algorithm. In the case of a minigame that has a simple structure, the agent does not need human expert data for behavior cloning. However, in the case of an actual 1 versus 1 game, the training time increases exponentially due to excessive numbers of states and actions. In this post, I will check that the Alpha Star Model can actually mimic the play of humans using human replay data.

Warm-Up
The simplest method for Behavior Cloning is to use the difference between the output action of the network at the replay state and the replay action as the loss value. First, try to do an experiment with the LunarLander-v2 environment of OpenAI Gym, which is a simple environment, to see if this method actually works for Actor-Critic Reinforcement Learning.

The code and dataset for that explanation can be downloaded from Google Drive.

First, I change the environment, network size of the Actor-Critic official code of Tensorflow and train it until the reward reaches 200. I use this trained model to create expert data using the below code.

Code for collecting the expert data of A2C for Behavior Cloning
Please try to understand every part of the original code because I reuse code of it to make a Supervised Learning part. Next, the loss is calculated from the difference between the action of expert data and the action of the network.

Supervised loss for A2C
Unlike the loss of Reinforcement Learning, there is no loss for the critic network calculated by reward. You can start Supervised Learning by running the below cell of the shared Jupyter Notebook file.

The main loop of A2C for Behavior Cloning
The train_supervised_step is a function corresponding to the train_step function of the original code. You can train agents using the Reinforcement Learning method after finishing behavior cloning.

The train_supervised_step function of A2C for behavior cloning
The run_supervised_episode function corresponds to the run_episode function that collects the data from the actual environment. This function collects expert data for Supervised Learning from the npy type files that we made before.

The run_supervised_episode function of A2C for Behavior Cloning
We can use the newly created Supervised Learning to train the Reinforcement Learning. First, save the action logit of model what we are going to train and the supervised model together.

Run Reinforcement Episode with Supervised Learning model
The Kl Divergence method can be used to calculate the difference between action logits because they are a probabilistic distribution.

Computing KL Loss
The KL loss calculated is added to the RL loss to train the model.

Training Reinforcement Learning with KL loss
In this way, the agent can learn faster than using Reinforcement Learning alone, as shown in the graph below.


Reward Graph of Supervised A2C
Supervised Learning for Starcraft 2
In the case of the LunarLander-v2 environment, it can be solved sufficiently by using only Reinforcement Learning. However, it is common to use Supervised Learning together as in the case of Starcraft 2 which has complicated observation and action space to shorten the training time. In the case of the LunarLander-v2 environment, data can be generated using the model trained from Reinforcement Learning, and loss is calculated for only one action. However, for Starcraft 2 case, humans need to play the game directly to collect data, and action type and all 13 action arguments need to be considered for loss.

Replay file of Terran vs Terran at Simple64 map using two barracks marine rush: https://drive.google.com/drive/folders/1lqb__ubLKLfw4Jiig6KsO-D0e_wrnGWk
Supervised Training code for PySC2: https://github.com/kimbring2/AlphaStar_Implementation
After downloading the replay file, you need to convert it to hkl file format. See https://github.com/kimbring2/AlphaStar_Implementation#supervised-training for detailed instructions.

The code below basically uses One-Hot Encoding, Categorical Cross-Entropy, and L2 Regularization to calculate the loss for Supervised Learning in Pysc2, similar to the LunarLander-v2 environment.

The loss function of PySC2 Supervised Learning
However, in the case of Starcraft2, even if a human executes a specific action type, it may not be reflected depending on whether it is available in the current game. Therefore, it is filtered via the available_actions feature. Furthermore, even in the case of the action argument, it is also filtered because specific action types only need a limited action argument.

The following graph is a loss of Supervised Learning for Starcraft2. Due to the complexity of action and observation space, it takes a very long time to train.


Training loss graph of Starcraft 2
The following video is the evaluation result of the trained model around 5 days using about 1000 expert data. We can confirm that agent imitates human behavior to some level.

Evaluation of Supervised Learning of Starcraft 2
Conclusion
In this post, we investigate the Supervised Learning method using expert data in Actor-Critic Reinforcement Learning in the case of Starcraft 2. Considering that almost all actual environments outside the laboratory are complicated. Human data and loss calculation for multiple action methods should be considered essential for agent development of high performance.

Starcraft 2
Deep Learning
TensorFlow
Rtstrategy
15



Nerd For Tech
Published in Nerd For Tech
11.8K Followers
·
Last published Feb 1, 2025
NFT is an Educational Media House. Our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. To know more about us, visit https://www.nerdfortech.org/.

Follow
Dohyeong Kim
Written by Dohyeong Kim
104 Followers
·
97 Following
I am a Deep Learning researcher. Currently, I am trying to make an AI agent for various situations such as MOBA, RTS, and Soccer games.

Follow
No responses yet
What are your thoughts?

Cancel
Respond
Respond

Also publish to my profile

More from Dohyeong Kim and Nerd For Tech
Implementation of the Hide and Seek of the OpenAI — Part 1
Dohyeong Kim
Dohyeong Kim

Implementation of the Hide and Seek of the OpenAI — Part 1
Collaboration is an essential function of multiplayer game such as a MOBA, and Soccer game. In the case of Reinforcement Learning, the…
Mar 12, 2023
Don’t use div — Have Some Empathy
Nerd For Tech
In

Nerd For Tech

by

Saravanan M

Don’t use div — Have Some Empathy
How mindlessly using <div> can make some life harder.

Sep 28, 2024
20 Practical Ways to Use Tuples in C# to Boost Your .NET Productivity
Nerd For Tech
In

Nerd For Tech

by

AshokReddy

20 Practical Ways to Use Tuples in C# to Boost Your .NET Productivity
Introduction:

Jan 25
How to build the Deep Learning agent for Minecraft with code— Tutorial 2
Dohyeong Kim
Dohyeong Kim

How to build the Deep Learning agent for Minecraft with code— Tutorial 2
In the previous post, I created two model needed to create Deep Learning agent for Minecraft.
Mar 5, 2022
See all from Dohyeong Kim
See all from Nerd For Tech
Recommended from Medium
AI Agents: Build an Agent from Scratch (Part-2)
Vipra Singh
Vipra Singh

AI Agents: Build an Agent from Scratch (Part-2)
Discover AI agents, their design, and real-world applications.

3d ago
A Rough Framework for Machine Learning
Artificial Intelligence in Plain English
In

Artificial Intelligence in Plain English

by

Ashish Negi

A Rough Framework for Machine Learning
Learn a basic framework for machine learning, covering essential steps to build and optimize models for better performance and accuracy.

Jan 21
Lists
Principal Component Analysis for ML
Time Series Analysis
deep learning cheatsheet for beginner
Practical Guides to Machine Learning
10 stories
·
2209 saves



Natural Language Processing
1954 stories
·
1599 saves



data science and AI
40 stories
·
333 saves


Staff picks
815 stories
·
1627 saves
Building Multi-Agents Supervisor System from Scratch with LangGraph & Langsmith
Anurag Mishra
Anurag Mishra

Building Multi-Agents Supervisor System from Scratch with LangGraph & Langsmith
The Rise of Multi-Agent Systems: The Third Wave of AI
4d ago
Easily Enhance your Tabular Data Models with DeepTables
Writing in the World of Artificial Intelligence
In

Writing in the World of Artificial Intelligence

by

Abish Pius

Easily Enhance your Tabular Data Models with DeepTables
In the domain of click-through rate (CTR) prediction and other predictive tasks, numerous models have been developed, each outperforming…

Oct 4, 2024
Goodbye RAG? Gemini 2.0 Flash Have Just Killed It!
Everyday AI
In

Everyday AI

by

Manpreet Singh

Goodbye RAG? Gemini 2.0 Flash Have Just Killed It!
Alright!!!

Feb 10
I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market
DataDrivenInvestor
In

DataDrivenInvestor

by

Austin Starks

I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market
It literally took one try. I was shocked.

Sep 16, 2024
See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Terms

Text to speech

Teams